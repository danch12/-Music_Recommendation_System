{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I didnt know how to exactly include the google collab notebook that I used into github so i copy and pasted the code into a jupyter notebook here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T11:37:47.687632Z",
     "start_time": "2020-04-08T11:37:46.211911Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Conv2D,AveragePooling2D, BatchNormalization,Dropout,Flatten,Dense\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "import tensorflow.keras\n",
    "import pickle\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing import image as image_utils\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks if gpu enabled\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T17:38:58.760004Z",
     "start_time": "2020-03-27T17:38:38.297Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('new_spectros.pkl', 'rb') as f:\n",
    "     spectros= pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T17:38:58.765507Z",
     "start_time": "2020-03-27T17:38:38.435Z"
    }
   },
   "outputs": [],
   "source": [
    "#load in spectrograms\n",
    "techno_arr = np.asarray(spectros[0])\n",
    "pop_arr= np.asarray(spectros[1])\n",
    "hip_arr = np.asarray(spectros[2])\n",
    "garage_arr = np.asarray(spectros[3])\n",
    "rock_arr=np.asarray(spectros[5])\n",
    "classical_arr =np.asarray(spectros[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T17:38:58.775135Z",
     "start_time": "2020-03-27T17:38:38.587Z"
    }
   },
   "outputs": [],
   "source": [
    "X=np.concatenate((techno_arr,pop_arr,hip_arr,garage_arr,rock_arr,classical_arr),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T17:38:58.777839Z",
     "start_time": "2020-03-27T17:38:38.732Z"
    }
   },
   "outputs": [],
   "source": [
    "X_s=X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T17:38:58.786282Z",
     "start_time": "2020-03-27T17:38:38.893Z"
    }
   },
   "outputs": [],
   "source": [
    "genre_dict = {\n",
    "              'techno':0,\n",
    "              'pop':1,\n",
    "              'hip_hop':2,\n",
    "              'garage':3,\n",
    "              'rock':4,\n",
    "              'classical':5\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T17:38:58.801778Z",
     "start_time": "2020-03-27T17:38:39.156Z"
    }
   },
   "outputs": [],
   "source": [
    "techno_y = np.repeat(0, techno_arr.shape[0]).reshape(-1,1)\n",
    "pop_y = np.repeat(1, pop_arr.shape[0]).reshape(-1,1)\n",
    "hip_hop_y =np.repeat(2,hip_arr.shape[0]).reshape(-1,1)\n",
    "garage_y = np.repeat(3,garage_arr.shape[0]).reshape(-1,1)\n",
    "rock_y =np.repeat(4,rock_arr.shape[0]).reshape(-1,1)\n",
    "classical_y =np.repeat(5,classical_arr.shape[0]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T17:38:58.806838Z",
     "start_time": "2020-03-27T17:38:39.446Z"
    }
   },
   "outputs": [],
   "source": [
    "ys = np.concatenate( (techno_y,pop_y,hip_hop_y,garage_y,rock_y,classical_y),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T17:38:58.810645Z",
     "start_time": "2020-03-27T17:38:39.745Z"
    }
   },
   "outputs": [],
   "source": [
    "y_try = ys.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T17:38:58.816948Z",
     "start_time": "2020-03-27T17:38:40.044Z"
    }
   },
   "outputs": [],
   "source": [
    "#turn to one hot encoded for model\n",
    "y_one_hot= to_categorical(y_try[0])\n",
    "y_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T17:38:58.822068Z",
     "start_time": "2020-03-27T17:38:40.303Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train,X_test, y_train, y_test= train_test_split(X_s,y_one_hot,test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Own Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "                                tf.keras.layers.Conv1D(64, 7, activation=\"relu\", padding=\"same\",\n",
    "                        input_shape=(128, 644)),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.AveragePooling1D( pool_size=4,strides=2 ),\n",
    "                        tf.keras.layers.Conv1D(128, 7,strides=2, activation=\"relu\", padding=\"same\"\n",
    "                        ),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.AveragePooling1D(pool_size=2,strides=2),\n",
    "                        tf.keras.layers.Conv1D(512,3, activation ='relu'),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.AveragePooling1D(pool_size=2,strides=2),\n",
    "                        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "                        tf.keras.layers.Flatten(),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                        tf.keras.layers.Dropout(0.5),\n",
    "                        tf.keras.layers.Dense(1024),\n",
    "                        tf.keras.layers.LeakyReLU(alpha=0.05),\n",
    "                        tf.keras.layers.Dropout(0.5),\n",
    "                        tf.keras.layers.Dense(256),\n",
    "                        tf.keras.layers.LeakyReLU(alpha=0.05),\n",
    "                        tf.keras.layers.Dropout(0.25),\n",
    "                        tf.keras.layers.Dense(80),\n",
    "                        tf.keras.layers.LeakyReLU(alpha=0.05),\n",
    "                        tf.keras.layers.Dense(40),\n",
    "                        tf.keras.layers.LeakyReLU(alpha=0.05),\n",
    "                        tf.keras.layers.Dense(6,activation='softmax')\n",
    "]\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to expand dimensions for model to accept\n",
    "X_dim = X.reshape(X.shape[0],X.shape[1],X.shape[2],1)\n",
    "X_dim = X_dim / 255\n",
    "X_train,X_test, y_train, y_test= train_test_split(X_dim,y_one_hot,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T17:38:58.837842Z",
     "start_time": "2020-03-27T17:38:40.452Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.resize(X_train.shape[0],X_train.shape[1],X_train.shape[2],3)\n",
    "X_test_resized=np.resize(X_test,(X_test.shape[0],X_test.shape[1],X_test.shape[2],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn into a tf dataset so the model doesnt take 10 million years\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_resized, y_test))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T17:38:58.840162Z",
     "start_time": "2020-03-27T17:38:40.616Z"
    }
   },
   "outputs": [],
   "source": [
    "#build the model\n",
    "vgg16_conv = tf.keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                  include_top=False,\n",
    "                                                   input_shape=(128, 644, 3))\n",
    "\n",
    "avg = tf.keras.layers.GlobalAveragePooling1D()(vgg16_conv.output)\n",
    "flat = tf.keras.layers.Flatten()(avg)\n",
    "dense1 = tf.keras.layers.Dense(1024)(flat)\n",
    "leak = tf.keras.layers.LeakyReLU(alpha=0.05)(dense1)\n",
    "drop = tf.keras.layers.Dropout(0.5)(leak)\n",
    "dense2 =tf.keras.layers.Dense(256)(drop)\n",
    "leak2 = tf.keras.layers.LeakyReLU(alpha=0.05)(dense2)\n",
    "drop2 = tf.keras.layers.Dropout(0.5)(leak2)\n",
    "dense3 =tf.keras.layers.Dense(80)(drop2)\n",
    "leak3 = tf.keras.layers.LeakyReLU(alpha=0.05)(dense3)\n",
    "dense4= tf.keras.layers.Dense(40)(leak3)\n",
    "leak4 = tf.keras.layers.LeakyReLU(alpha=0.05)(dense4)\n",
    "output =  tf.keras.layers.Dense(7, activation='softmax', name='predictions')(leak4)\n",
    "model = tf.keras.Model(inputs=vgg16_conv.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T17:38:58.844358Z",
     "start_time": "2020-03-27T17:38:40.784Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup a model checkpoint to save our best model\n",
    "\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '/content/drive/My Drive/Spotify_music_classification/models/vgg{epoch:02d}-{val_loss:.2f}.h5',\n",
    "        \n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T17:38:58.847431Z",
     "start_time": "2020-03-27T17:38:40.975Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T17:38:58.868913Z",
     "start_time": "2020-03-27T17:38:41.610Z"
    }
   },
   "outputs": [],
   "source": [
    "#freeze the base model weights\n",
    "for layer in vgg16_conv.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T17:38:58.871345Z",
     "start_time": "2020-03-27T17:38:41.790Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T17:38:58.873597Z",
     "start_time": "2020-03-27T17:38:41.976Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T17:38:58.877314Z",
     "start_time": "2020-03-27T17:38:42.131Z"
    }
   },
   "outputs": [],
   "source": [
    "#train model\n",
    "history = model.fit(\n",
    "    X_train,y_train,verbose=1,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=3527,\n",
    "    validation_data=(X_test_resized,y_test),shuffle=True,\n",
    "    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg16_conv.layers:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,y_train,verbose=1,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=3527,\n",
    "    validation_data=(X_test_resized,y_test),shuffle=True,\n",
    "    callbacks=[checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
